{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Manually Compute Gradients</H2>\n",
    "<p>Welcome!</p>\n",
    "<p>This code is meant to be written with greater detail and explanation than normal in order to help new Python/ML programmers get passed some hurdles.  </p>\n",
    "\n",
    "<p>The code snippets come from a combination of O'Reilly's \"Hands-On Machine Learning with Scikit-Learn & TensorFlow\" as well as my own combined.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m value =  20640\n",
      "n value =  8\n",
      "Shape before bias: (20640, 8)\n",
      "Shape after bias added:  (20640, 9)\n",
      "[  1.00000000e+00   8.30140000e+00   2.10000000e+01   6.23813708e+00\n",
      "   9.71880492e-01   2.40100000e+03   2.10984183e+00   3.78600000e+01\n",
      "  -1.22220000e+02]\n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing() # sklearn makes using data sets easy!\n",
    "m, n = housing.data.shape # Unlike C++, you can return multiple values from functions\n",
    "\n",
    "print(\"m value = \" , m)\n",
    "print(\"n value = \", n)\n",
    "\n",
    "\n",
    "#Print housing.data.shape before augmenting it with a bias\n",
    "print(\"Shape before bias:\", housing.data.shape)\n",
    "\n",
    "# Adding bias: concatenating a vector of \"1\"s with length m\n",
    "# to the housing.data data. It adds a bias \"feature\"\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "# Just to show that a feature was infact added tot he housing.data\n",
    "print(\"Shape after bias added: \", housing_data_plus_bias.shape)\n",
    "\n",
    "# Print out the first row of the dataset\n",
    "print(housing_data_plus_bias[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000 # Number of times to traverse entire dataset\n",
    "learning_rate = 0.01 # scaled down changes to weights. If a change of 10 was calculated, use 10*.01 instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember when dealing with neural networks, it's always best to scale the data. StandardScaler removes the mean and scales the data to unit variance. What this means is that it take the average of the feature values, each column, and subracts it from the actual value. Thus, making the new average = 0. Then dividing it by the std deviation to reduce the range. \n",
    "\n",
    "Why do this? Think about data centered around the origin vs data that is say, in the top right corner of your coordinate system. Now draw a line through the data to separate two classes as best you can. If your data is very, very far away, each wiggle or bit you are off on that line drawn will have a scaled effect on how you classify your data. If your data was centered about the origin and you draw such a line, you have much better \"wiggle\" room. This is probably not the greatest analogy without pictures, sorry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640,)\n",
      "(20640, 1)\n",
      "Epoch 0 MSE = 5.67843\n",
      "Epoch 100 MSE = 4.88732\n",
      "Epoch 200 MSE = 4.8513\n",
      "Epoch 300 MSE = 4.83743\n",
      "Epoch 400 MSE = 4.82801\n",
      "Epoch 500 MSE = 4.82122\n",
      "Epoch 600 MSE = 4.81631\n",
      "Epoch 700 MSE = 4.81276\n",
      "Epoch 800 MSE = 4.81017\n",
      "Epoch 900 MSE = 4.8083\n",
      "(9, 1)\n",
      "[[  9.04542923e-01]\n",
      " [  7.74078131e-01]\n",
      " [  1.31192401e-01]\n",
      " [ -1.17845133e-01]\n",
      " [  1.64778173e-01]\n",
      " [  7.44095247e-04]\n",
      " [ -3.91945131e-02]\n",
      " [ -8.61356437e-01]\n",
      " [ -8.23479652e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_housing_data = scaler.fit_transform(housing_data_plus_bias.data)\n",
    "\n",
    "# Make sure the matrix scaled_housing_data is all of type tf.float32 and store it as X (inputs)\n",
    "X = tf.constant(scaled_housing_data, dtype=tf.float32, name=\"X\")\n",
    "\n",
    "print(housing.target.shape)\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "print(y.shape)\n",
    "\n",
    "# Creat reandom variables of size (n+1, 1) with min -1 and max 1.0. Seed 42 to make random numbers repeatable\n",
    "# remember n = 8 so this is a 9 row, 1 column matrix\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "\n",
    "error = y_pred - y\n",
    "\n",
    "# The average of the errors^2 or mse\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\") # cost function or loss function\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "\n",
    "# operate on the original theta values to reduce the loss\n",
    "training_op = tf.assign(theta, theta-learning_rate*gradients)\n",
    "\n",
    "init = tf.global_variables_initializer() # initialize all variables\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) #  run the session with all the initialized variables\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval()) # Print MSE every 100 iterations\n",
    "        sess.run(training_op) # Apply the training_op each iteration\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(best_theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
